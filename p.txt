# games_data.py

import pandas as pd
import numpy as np
import os

class GamesData:
    """
    Manages data pipeline for training and prediction:
    - Loads 'games_data.csv'
    - Computes team average stats
    - Merges home/away rows into one row per game
    - Creates difference & ratio features
    - Provides final (X, y)
    """

    # The columns we average for each team
    _cols_to_avg = [
        "FGA_2", "FGM_2", "FGA_3", "FGM_3",
        "FTA",   "FTM",   "AST",   "BLK",
        "STL",   "TOV",   "DREB",  "OREB",
        "F_personal"
    ]

    def __init__(self, csv_path: str):
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"{csv_path} does not exist.")
        self.csv_path = csv_path
        self.df = pd.read_csv(csv_path)

        # Precompute the team averages
        self.team_avgs = self._compute_team_averages(self.df)

        # We'll define a base set of columns for features
        # (home_avg_*), (away_avg_*), plus rest/travel
        self.base_feature_cols = []
        for col in self._cols_to_avg:
            self.base_feature_cols.append(f"home_avg_{col}")
        for col in self._cols_to_avg:
            self.base_feature_cols.append(f"away_avg_{col}")

        self.base_feature_cols += [
            "home_rest_days", "home_travel_dist",
            "away_rest_days", "away_travel_dist"
        ]

        # We'll store final feature column names in self.FEATURE_COLS
        # This will include difference & ratio columns
        self.FEATURE_COLS = list(self.base_feature_cols)  # start with base
        # We'll add difference/ratio columns dynamically after we see data

    def _compute_team_averages(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Group by 'team' and compute mean of self._cols_to_avg
        """
        df_avg = df.groupby("team")[self._cols_to_avg].mean().reset_index()
        rename_dict = {col: f"avg_{col}" for col in self._cols_to_avg}
        df_avg.rename(columns=rename_dict, inplace=True)
        return df_avg

    def prepare_training_data(self) -> pd.DataFrame:
        """
        Merges home & away rows into one row per game, storing
        home_* stats, away_* stats, plus the label 'home_team_won'.
        """
        df_team_avgs = self.team_avgs

        df_home = self.df[self.df["home_away"] == "home"].copy()
        df_away = self.df[self.df["home_away"] == "away"].copy()

        # Merge each side with the team averages
        df_home = df_home.merge(df_team_avgs, on="team", how="left")
        df_away = df_away.merge(df_team_avgs, on="team", how="left")

        # Rename columns with "home_" or "away_"
        home_cols = {}
        away_cols = {}
        for col in df_home.columns:
            if col not in ["game_id", "team", "game_date", "home_away"]:
                home_cols[col] = f"home_{col}"

        for col in df_away.columns:
            if col not in ["game_id", "team", "game_date", "home_away"]:
                away_cols[col] = f"away_{col}"

        df_home.rename(columns=home_cols, inplace=True)
        df_away.rename(columns=away_cols, inplace=True)

        # Merge home & away frames on 'game_id'
        merged = pd.merge(df_home, df_away, on="game_id", how="inner")

        # Label: did the home team win?
        merged["home_team_won"] = (merged["home_team_score"] > merged["away_team_score"]).astype(int)

        return merged

    def get_feature_and_label_arrays(self, merged_df: pd.DataFrame, label_col: str = "home_team_won"):
        """
        1) Extract initial feature array from base_feature_cols
        2) Add difference & ratio features
        3) Return (X, y)
        """
        # 1) Build a DataFrame with the base features
        # (We must ensure these columns exist in merged_df)
        for col in self.base_feature_cols:
            if col not in merged_df.columns:
                merged_df[col] = 0.0  # fill missing

        X_base = merged_df[self.base_feature_cols].values
        y = merged_df[label_col].values

        # 2) Create difference & ratio features dynamically
        # We'll do them for each stat in self._cols_to_avg
        # e.g. diff_FGA_2 = home_avg_FGA_2 - away_avg_FGA_2
        #      ratio_FGA_2 = (home_avg_FGA_2 + 1) / (away_avg_FGA_2 + 1)
        # The +1 avoids division by zero
        diff_ratio_features = []
        diff_ratio_names = []

        # We'll build them for each row in X_base
        # We'll need the columns in the correct order:
        # home_avg_FGA_2 is index 0 in the first block,
        # away_avg_FGA_2 is index (len(self._cols_to_avg)) in the second block
        # Then we have rest/travel. We'll isolate stats first to do diffs

        n_stats = len(self._cols_to_avg)  # e.g. 13
        # Indices for home stats: 0..(n_stats-1)
        # Indices for away stats: n_stats..(2*n_stats-1)

        for i_row in range(X_base.shape[0]):
            row_vals = X_base[i_row]
            # We'll create a place to store difference+ratio for each stat
            row_diff_ratio = []
            for i_stat in range(n_stats):
                home_val = row_vals[i_stat]
                away_val = row_vals[n_stats + i_stat]

                diff = home_val - away_val
                ratio = (home_val + 1e-6) / (away_val + 1e-6)  # avoid zero

                row_diff_ratio.append(diff)
                row_diff_ratio.append(ratio)

            diff_ratio_features.append(row_diff_ratio)

        # Build the final array of difference & ratio features
        diff_ratio_features = np.array(diff_ratio_features, dtype=np.float32)

        # Now let's define the names for these columns
        for stat_col in self._cols_to_avg:
            diff_name = f"diff_{stat_col}"
            ratio_name = f"ratio_{stat_col}"
            diff_ratio_names.append(diff_name)
            diff_ratio_names.append(ratio_name)

        # 3) Combine X_base with these new features
        X = np.concatenate([X_base, diff_ratio_features], axis=1)

        # Update self.FEATURE_COLS if not done yet
        # Base plus the new difference & ratio columns
        if len(self.FEATURE_COLS) == len(self.base_feature_cols):
            self.FEATURE_COLS += diff_ratio_names

        return X, y

    def build_prediction_features(self,
                                  team_home: str,
                                  team_away: str,
                                  home_rest_days: float,
                                  home_travel_dist: float,
                                  away_rest_days: float,
                                  away_travel_dist: float) -> np.ndarray:
        """
        Creates a single 1-row feature vector for a new matchup.
        1) Look up home & away average stats
        2) Insert into base feature array
        3) Add difference & ratio features
        """
        # 1) Retrieve home/away average stats
        home_avgs = self.team_avgs[self.team_avgs["team"] == team_home]
        away_avgs = self.team_avgs[self.team_avgs["team"] == team_away]

        if home_avgs.empty:
            raise ValueError(f"No average stats found for home team '{team_home}'")
        if away_avgs.empty:
            raise ValueError(f"No average stats found for away team '{team_away}'")

        # For convenience, let's create a dictionary of required fields
        features_dict = {}

        # Insert home avg stats
        for col in self._cols_to_avg:
            home_val = float(home_avgs.iloc[0][f"avg_{col}"])
            features_dict[f"home_avg_{col}"] = home_val
        # Insert away avg stats
        for col in self._cols_to_avg:
            away_val = float(away_avgs.iloc[0][f"avg_{col}"])
            features_dict[f"away_avg_{col}"] = away_val

        # Insert rest/travel
        features_dict["home_rest_days"] = home_rest_days
        features_dict["home_travel_dist"] = home_travel_dist
        features_dict["away_rest_days"] = away_rest_days
        features_dict["away_travel_dist"] = away_travel_dist

        # Build the base array
        row_base = []
        for col in self.base_feature_cols:
            row_base.append(features_dict.get(col, 0.0))

        row_base = np.array(row_base, dtype=np.float32).reshape(1, -1)

        # 2) Now build difference+ratio features for the single row
        n_stats = len(self._cols_to_avg)
        diffratio_row = []
        for i_stat, stat_col in enumerate(self._cols_to_avg):
            home_val = row_base[0][i_stat]
            away_val = row_base[0][n_stats + i_stat]

            diff = home_val - away_val
            ratio = (home_val + 1e-6) / (away_val + 1e-6)
            diffratio_row.append(diff)
            diffratio_row.append(ratio)

        diffratio_array = np.array(diffratio_row, dtype=np.float32).reshape(1, -1)

        # 3) Concatenate base + difference/ratio
        X_single = np.concatenate([row_base, diffratio_array], axis=1)
        return X_single

# test_model.py

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
from sklearn.metrics import (
    accuracy_score, brier_score_loss, log_loss, roc_auc_score,
    confusion_matrix, classification_report, roc_curve, precision_recall_curve
)

from model import Model
from games_data import GamesData

def main():
    # 1) Create the GamesData (for consistent feature engineering)
    training_csv_path = "games_data.csv"
    games_data = GamesData(training_csv_path)

    # 2) Load the pre-trained Stacking Model
    model_file_path = "model_stuff.json"
    my_model = Model(model_file_path=model_file_path, games_data=games_data)
    if my_model.model is None:
        raise ValueError(f"No model loaded from {model_file_path}.")

    # 3) Example single-game prediction
    prob = my_model.predict_single_game(
        team_home="lsu_tigers",
        team_away="georgia_lady_bulldogs",
        home_rest_days=3.0,
        home_travel_dist=100.0,
        away_rest_days=2.0,
        away_travel_dist=200.0
    )
    print(f"\nSingle-Game Prediction: Probability home wins = {prob:.4f}")

    # 4) Evaluate entire test CSV
    test_csv_path = "games_data_testing.csv"
    if not os.path.exists(test_csv_path):
        print(f"No {test_csv_path} found, skipping batch evaluation.")
        return

    test_df = pd.read_csv(test_csv_path)
    print(f"\nLoaded {len(test_df)} rows from {test_csv_path}...")

    y_probs = []
    y_true = []

    has_scores = ("team_score_Home" in test_df.columns
                  and "team_score_Away" in test_df.columns)

    for _, row in test_df.iterrows():
        team_home = row.get("team_home", "")
        team_away = row.get("team_away", "")

        # We look for "home_rest_days"/"rest_days_Home"
        home_rest_days = row.get("home_rest_days", row.get("rest_days_Home", 0.0))
        away_rest_days = row.get("away_rest_days", row.get("rest_days_Away", 0.0))
        home_travel_dist = row.get("home_travel_dist", row.get("travel_dist_Home", 0.0))
        away_travel_dist = row.get("away_travel_dist", row.get("travel_dist_Away", 0.0))

        # Predict
        prob_home_win = my_model.predict_single_game(
            team_home=team_home,
            team_away=team_away,
            home_rest_days=home_rest_days,
            home_travel_dist=home_travel_dist,
            away_rest_days=away_rest_days,
            away_travel_dist=away_travel_dist
        )
        y_probs.append(prob_home_win)

        if has_scores:
            home_score = row["team_score_Home"]
            away_score = row["team_score_Away"]
            y_true.append(1 if home_score > away_score else 0)

    y_probs = np.array(y_probs)

    # If we have actual scores, evaluate
    if has_scores and len(y_true) == len(y_probs):
        y_true = np.array(y_true)
        # Convert probabilities to 0/1
        y_pred = (y_probs >= 0.5).astype(int)

        accuracy = accuracy_score(y_true, y_pred)
        brier = brier_score_loss(y_true, y_probs)
        ll = log_loss(y_true, np.column_stack([1 - y_probs, y_probs]))
        auc = roc_auc_score(y_true, y_probs)
        cm = confusion_matrix(y_true, y_pred)
        report = classification_report(y_true, y_pred, digits=4)

        print("\n=== MODEL PERFORMANCE (TEST) ===")
        print(f"Accuracy:  {accuracy:.4f}")
        print(f"Brier:     {brier:.4f}")
        print(f"Log Loss:  {ll:.4f}")
        print(f"ROC AUC:   {auc:.4f}")
        print("\nConfusion Matrix:")
        print(cm)
        print("\nClassification Report:")
        print(report)

        # ----------------------
        # Plot Additional Graphs
        # ----------------------
        # We'll create a figure with multiple subplots

        fig, axs = plt.subplots(2, 3, figsize=(18, 12))
        axs = axs.ravel()

        # 1) Probability vs. Actual Point Difference
        if "team_score_Home" in test_df.columns and "team_score_Away" in test_df.columns:
            actual_diff = test_df["team_score_Home"] - test_df["team_score_Away"]
            axs[0].scatter(y_probs, actual_diff, alpha=0.5)
            axs[0].set_xlabel("Predicted Probability (Home Win)")
            axs[0].set_ylabel("Actual Home Team Point Diff")
            axs[0].set_title("Prediction vs. Actual Point Diff")
            axs[0].grid(True)

        # 2) Calibration Plot
        n_bins = 10
        bins = np.linspace(0, 1, n_bins + 1)
        bin_indices = np.digitize(y_probs, bins) - 1

        actual_win_rates = []
        mean_predicted = []

        for i in range(n_bins):
            idxs = np.where(bin_indices == i)[0]
            if len(idxs) > 0:
                bin_mean_prob = np.mean(y_probs[idxs])
                bin_actual_win = np.mean(y_true[idxs])
            else:
                bin_mean_prob = np.nan
                bin_actual_win = np.nan

            mean_predicted.append(bin_mean_prob)
            actual_win_rates.append(bin_actual_win)

        axs[1].plot(mean_predicted, actual_win_rates, 'o-', label="Actual vs. Predicted")
        axs[1].plot([0, 1], [0, 1], 'k--', label="Perfect Calibration")
        axs[1].set_xlabel("Mean Predicted Probability")
        axs[1].set_ylabel("Actual Home Win Rate")
        axs[1].set_title("Calibration Plot")
        axs[1].legend(loc="best")
        axs[1].grid(True)

        # 3) ROC Curve
        fpr, tpr, thresholds_roc = roc_curve(y_true, y_probs)
        axs[2].plot(fpr, tpr, label=f"AUC = {auc:.3f}")
        axs[2].plot([0, 1], [0, 1], 'k--')
        axs[2].set_xlabel("False Positive Rate")
        axs[2].set_ylabel("True Positive Rate")
        axs[2].set_title("ROC Curve")
        axs[2].legend(loc="lower right")
        axs[2].grid(True)

        # 4) Precision-Recall Curve
        precision, recall, thresholds_pr = precision_recall_curve(y_true, y_probs)
        axs[3].plot(recall, precision, label="Precision-Recall")
        axs[3].set_xlabel("Recall")
        axs[3].set_ylabel("Precision")
        axs[3].set_title("Precision-Recall Curve")
        axs[3].legend(loc="best")
        axs[3].grid(True)

        # 5) Histogram of predicted probabilities
        axs[4].hist(y_probs, bins=20, range=(0,1), alpha=0.7, color='g', edgecolor='k')
        axs[4].set_xlabel("Predicted Probability (Home Win)")
        axs[4].set_ylabel("Count")
        axs[4].set_title("Distribution of Predictions")
        axs[4].grid(True)

        # 6) Confusion Matrix (visual)
        # We'll make a small heatmap of cm
        # But you can also do this in Seaborn if you want a nicer style
        # We'll just do a text-based image for demonstration
        axs[5].imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
        axs[5].set_title("Confusion Matrix (Visual)")
        axs[5].set_xticks([0, 1])
        axs[5].set_yticks([0, 1])
        axs[5].set_xticklabels(["Pred 0", "Pred 1"])
        axs[5].set_yticklabels(["True 0", "True 1"])

        # Write the counts inside the heatmap
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                axs[5].text(j, i, str(cm[i, j]),
                            ha="center", va="center",
                            color="black", fontsize=12)

        axs[5].set_ylabel("True label")
        axs[5].set_xlabel("Predicted label")

        plt.tight_layout()
        plt.show()


if __name__ == "__main__":
    main()

# generate_model.py

from model import Model
from games_data import GamesData

def main():
    # 1) Create a GamesData instance for 'games_data.csv'
    training_csv_path = "games_data.csv"
    games_data = GamesData(training_csv_path)

    # 2) Create the Model
    model_file_path = "model_stuff.json"
    my_model = Model(model_file_path=model_file_path, games_data=games_data)

    # 3) Generate (train) the model, doing hyperparameter search
    print("Training model (with advanced hyperparameter search)...")
    my_model.generate_model(do_hyperparam_search=True)
    print("Done training model.")

    # 4) Save the model
    my_model.save_model()
    print(f"Saved model to '{model_file_path}'.")

if __name__ == "__main__":
    main()

# model.py

import os
import numpy as np
import joblib
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.ensemble import StackingClassifier, RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, log_loss, brier_score_loss, roc_auc_score

import xgboost as xgb
from xgboost import XGBClassifier

from games_data import GamesData

class Model:
    def __init__(self, model_file_path: str, games_data: GamesData):
        """
        :param model_file_path: path to save or load model via joblib
        :param games_data: the GamesData instance providing features
        """
        self.model_file_path = model_file_path
        self.model = None
        self.games_data = games_data

        if os.path.exists(self.model_file_path):
            self.load_model()
        else:
            print(f"No existing model found at {self.model_file_path}. "
                  "Need to train a new one.")

    def generate_model(self, do_hyperparam_search=True):
        """
        Train stacking model with possible hyperparameter search
        for XGB and RandomForest.
        """
        # 1) Prepare data
        merged_df = self.games_data.prepare_training_data()
        X, y = self.games_data.get_feature_and_label_arrays(merged_df)

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )

        # 2) Define base estimators
        xgb_clf = XGBClassifier(
            eval_metric='logloss',
            random_state=42,
        )
        rf_clf = RandomForestClassifier(random_state=42)
        meta_learner = LogisticRegression(max_iter=2000)

        if do_hyperparam_search:
            # Hyperparameter search for XGB
            xgb_params = {
                "n_estimators":    [100, 200, 300, 500, 800, 1000],
                "max_depth":       [3, 4, 6, 8, 10],
                "learning_rate":   [0.01, 0.02, 0.05, 0.1],
                "subsample":       [0.6, 0.8, 1.0],
                "colsample_bytree":[0.6, 0.8, 1.0],
                "gamma":           [0, 0.1, 0.2, 0.5],
                "reg_lambda":      [1, 2, 5, 10],
                "reg_alpha":       [0, 0.1, 1, 2]
            }
            xgb_search = RandomizedSearchCV(
                estimator=xgb_clf,
                param_distributions=xgb_params,
                n_iter=30,  # increase for more thorough search
                scoring='roc_auc',
                cv=3,
                random_state=42,
                n_jobs=-1,
                verbose=1
            )
            xgb_search.fit(X_train, y_train)
            xgb_clf = xgb_search.best_estimator_
            print("Best XGB params:", xgb_search.best_params_)

            # Hyperparameter search for RF
            rf_params = {
                "n_estimators": [100, 200, 500, 800],
                "max_depth":    [None, 5, 10, 20, 30],
                "max_features": ["sqrt", "log2", None]
            }
            rf_search = RandomizedSearchCV(
                estimator=rf_clf,
                param_distributions=rf_params,
                n_iter=20,
                scoring='roc_auc',
                cv=3,
                random_state=42,
                n_jobs=-1,
                verbose=1
            )
            rf_search.fit(X_train, y_train)
            rf_clf = rf_search.best_estimator_
            print("Best RF params:", rf_search.best_params_)

        # 3) Build stacking classifier
        estimators = [
            ('xgb', xgb_clf),
            ('rf', rf_clf)
        ]
        stack_clf = StackingClassifier(
            estimators=estimators,
            final_estimator=meta_learner,
            cv=5,
            n_jobs=-1,
            passthrough=False
        )

        # 4) Train stacking
        print("\nTraining Stacking Classifier...")
        stack_clf.fit(X_train, y_train)
        self.model = stack_clf

        # 5) Evaluate on test set
        y_pred = self.model.predict(X_test)
        y_proba = self.model.predict_proba(X_test)[:, 1]

        acc = accuracy_score(y_test, y_pred)
        auc = roc_auc_score(y_test, y_proba)
        ll = log_loss(y_test, self.model.predict_proba(X_test))
        try:
            brier = brier_score_loss(y_test, y_proba)
        except:
            brier = None

        print(f"\n=== MODEL PERFORMANCE ON HOLD-OUT TEST ===")
        print(f"Accuracy:  {acc:.4f}")
        print(f"ROC AUC:   {auc:.4f}")
        print(f"Log Loss:  {ll:.4f}")
        if brier is not None:
            print(f"Brier:     {brier:.4f}")

        return self.model

    def predict_single_game(self,
                            team_home: str,
                            team_away: str,
                            home_rest_days: float,
                            home_travel_dist: float,
                            away_rest_days: float,
                            away_travel_dist: float) -> float:
        """
        For a single matchup, build the feature vector (including difference/ratio),
        then return the predicted probability that the home team wins.
        """
        if self.model is None:
            raise ValueError("No model loaded or trained. Please train or load a model first.")

        X_single = self.games_data.build_prediction_features(
            team_home, team_away,
            home_rest_days, home_travel_dist,
            away_rest_days, away_travel_dist
        )
        prob = self.model.predict_proba(X_single)[:, 1][0]
        return prob

    def predict_batch(self, X: np.ndarray) -> np.ndarray:
        """
        For batch predictions where X is already constructed,
        returns predicted probabilities for the home team.
        """
        if self.model is None:
            raise ValueError("No model found. Please train or load a model.")
        return self.model.predict_proba(X)[:, 1]

    def save_model(self):
        """
        Saves the trained model via joblib.
        """
        if self.model is None:
            print("No trained model to save.")
            return
        joblib.dump(self.model, self.model_file_path)
        print(f"Model saved to '{self.model_file_path}'")

    def load_model(self):
        """
        Loads the model from joblib if it exists.
        """
        self.model = joblib.load(self.model_file_path)
        print(f"Model loaded from '{self.model_file_path}'")

# games_data_testing.csv
game_id,team_home,team_away,rest_days_Home,rest_days_Away,travel_dist_Home,travel_dist_Away,team_score_Home,team_score_Away
game_2022_2011,georgia_lady_bulldogs,lsu_tigers,9.0,3.0,0.0,824.0,62,68
game_2022_2012,missouri_tigers,south_carolina_gamecocks,8.0,9.0,0.0,1154.0,70,69
game_2022_2013,tennessee_lady_volunteers,alabama_crimson_tide,3.0,17.0,0.0,439.0,62,44
game_2022_2111,alabama_crimson_tide,auburn_tigers,3.0,13.0,0.0,197.0,56,53
game_2022_2112,arkansas_razorbacks,tennessee_lady_volunteers,12.0,3.0,0.0,920.0,63,70
game_2022_2113,florida_gators,georgia_lady_bulldogs,12.0,3.0,0.0,484.0,69,73
game_2022_2114,lsu_tigers,texas_am_aggies,3.0,13.0,0.0,496.0,75,66

# games_data.csv
game_id,game_date,team,FGA_2,FGM_2,FGA_3,FGM_3,FTA,FTM,AST,BLK,STL,TOV,TOV_team,DREB,OREB,F_tech,F_personal,team_score,opponent_team_score,largest_lead,notD1_incomplete,OT_length_min_tot,rest_days,attendance,tz_dif_H_E,prev_game_dist,home_away,home_away_NS,travel_dist
game_2022_2011,12/30/2021,georgia_lady_bulldogs,50,22,11,5,6,3,14,7,7,18,0,25,11,0,18,62,68,1,FALSE,NA,9,3241,0,0,home,1,0
game_2022_2011,12/30/2021,lsu_tigers,50,24,11,4,15,8,15,2,15,14,2,25,11,0,7,68,62,14,FALSE,NA,3,3241,0,824,away,-1,824
game_2022_2012,12/30/2021,missouri_tigers,50,18,15,7,16,13,10,1,4,8,1,31,6,0,11,70,69,8,FALSE,5,8,6139,0,371,home,1,0
game_2022_2012,12/30/2021,south_carolina_gamecocks,50,23,21,6,9,5,15,8,3,8,0,27,20,0,15,69,70,6,FALSE,5,9,6139,0,1154,away,-1,1154
game_2022_2013,12/30/2021,tennessee_lady_volunteers,50,20,15,4,15,10,16,8,5,15,1,34,12,0,13,62,44,19,FALSE,NA,3,8124,0,0,home,1,0